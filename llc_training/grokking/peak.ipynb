{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to investigate how LLC estimation could be used as a training metric using the peak detection task.\n",
    "\n",
    "This is based off the Timaeus grokking notebook found here: https://github.com/timaeus-research/devinterp/blob/main/examples/grokking.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suJsw_j5eP9Y",
    "outputId": "17013fd2-6930-4305-af12-2a21e2feb98c"
   },
   "outputs": [],
   "source": [
    "%pip install devinterp nbformat\n",
    "%pip install devinterp[vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56I2aTSbeP9e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary\n",
    "from devinterp.utils import evaluate_ce\n",
    "from llc_training.grokking.peak_models import Nano, Small, Medium_Large\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvF8FyrLeP9g"
   },
   "outputs": [],
   "source": [
    "# Make all relevant functions and classes for training\n",
    "@dataclass\n",
    "class ExperimentParams:\n",
    "    p: int = 100\n",
    "    l: int = 5\n",
    "    v = 12\n",
    "    n_batches: int = 1000\n",
    "    n_save_model_checkpoints: int = 100\n",
    "    print_times: int = 100\n",
    "    lr: float = 3e-3\n",
    "    batch_size: int = 128\n",
    "    hidden_size: int = 48\n",
    "    linear_hidden_size: int = 200\n",
    "    embed_dim: int = 127\n",
    "    train_frac: float = 0.4\n",
    "    random_seed: int = 0\n",
    "    device: str = DEVICE\n",
    "    weight_decay: float = 2e-5\n",
    "    blocks: int = 6\n",
    "\n",
    "def test(model, dataset, device):\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataset:\n",
    "            x = x.tolist()\n",
    "            x = torch.tensor([x])\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)[0]\n",
    "            loss = loss_fn(out, y)\n",
    "            total_loss += loss.item()\n",
    "            for i in range(len(out)):\n",
    "                if y[i][0] > y[i][1] and out[i][0] > out[i][1]:\n",
    "                    n_correct += 1\n",
    "                if y[i][0] < y[i][1] and out[i][0] < out[i][1]:\n",
    "                    n_correct += 1\n",
    "    return n_correct / (5 * len(dataset)), total_loss / len(dataset)\n",
    "\n",
    "\n",
    "def train(train_dataset, test_dataset, params, verbose=True):\n",
    "    all_models = []\n",
    "    model = Medium_Large(params).to(params.device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=params.weight_decay, lr=params.lr\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "    print_every = params.n_batches // params.print_times\n",
    "    checkpoint_every = None\n",
    "    if params.n_save_model_checkpoints > 0:\n",
    "        checkpoint_every = params.n_batches // params.n_save_model_checkpoints\n",
    "\n",
    "    loss_data = []\n",
    "    if verbose:\n",
    "        pbar = tqdm(total=params.n_batches, desc=\"Training\")\n",
    "    for i in range(params.n_batches):\n",
    "        # Sample random batch of data\n",
    "        batch = next(iter(train_loader))\n",
    "        X, Y = batch\n",
    "        X, Y = X.to(params.device), Y.to(params.device)\n",
    "        # Gradient update\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = loss_fn(out, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if checkpoint_every and (i + 1) % checkpoint_every == 0:\n",
    "            all_models += [deepcopy(model)]\n",
    "\n",
    "        if (i + 1) % print_every == 0:\n",
    "            val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "            train_acc, train_loss = test(model, train_dataset, params.device)\n",
    "            loss_data.append(\n",
    "                {\n",
    "                    \"batch\": i + 1,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_acc\": val_acc,\n",
    "                }\n",
    "            )\n",
    "            if verbose:\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": f\"{train_loss:.4f}\",\n",
    "                        \"train_acc\": f\"{train_acc:.4f}\",\n",
    "                        \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                        \"val_acc\": f\"{val_acc:.4f}\",\n",
    "                    }\n",
    "                )\n",
    "                pbar.update(print_every)\n",
    "    if verbose:\n",
    "        pbar.close()\n",
    "    df = pd.DataFrame(loss_data)\n",
    "    train_acc, train_loss = test(model, train_dataset, params.device)\n",
    "    val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "    if verbose:\n",
    "        print(f\"Final Train Acc: {val_acc:.4f} | Final Train Loss: {val_loss:.4f}\")\n",
    "        print(f\"Final Val Acc: {val_acc:.4f} | Final Val Loss: {val_loss:.4f}\")\n",
    "    return all_models, df\n",
    "\n",
    "\n",
    "def deterministic_shuffle(lst, seed):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lst)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def make_dataset(p):\n",
    "    data = []\n",
    "    vocab = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    seq_len=5\n",
    "    for _ in range(p):\n",
    "        seq = [int(np.random.choice(vocab)) for _ in range(seq_len)]\n",
    "\n",
    "        peaks = []\n",
    "        for i in range(0, len(seq)):\n",
    "            if i == 0 or i == len(seq) - 1:\n",
    "                if len(seq) == 2:\n",
    "                    peaks.append(False)\n",
    "                elif i == 0:\n",
    "                    peaks.append(seq[i] > seq[i + 1])\n",
    "                elif i == len(seq) - 1:\n",
    "                    peaks.append(seq[i] > seq[i - 1])\n",
    "            else:\n",
    "                peaks.append(seq[i] > seq[i - 1] and seq[i] > seq[i + 1])\n",
    "        # Label = 1 if there is any peak, else 0\n",
    "        labels = []\n",
    "\n",
    "        for val in peaks:\n",
    "            if val == np.True_:\n",
    "                labels.append([0.0, 1.0])\n",
    "            else:\n",
    "                labels.append([1.0, 0.0])\n",
    "\n",
    "        data.append((torch.tensor(seq), torch.tensor(labels)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_test_split(dataset, train_split_proportion, seed):\n",
    "    l = len(dataset)\n",
    "    train_len = int(train_split_proportion * l)\n",
    "    idx = list(range(l))\n",
    "    idx = deterministic_shuffle(idx, seed)\n",
    "    train_idx = idx[:train_len]\n",
    "    test_idx = idx[train_len:]\n",
    "    return [dataset[i] for i in train_idx], [dataset[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xVu2IN7eP9o",
    "outputId": "da84ba29-4b2a-46c7-d4ea-9f5c4f338366"
   },
   "outputs": [],
   "source": [
    "# Initialize params and get the dataset\n",
    "params = ExperimentParams()\n",
    "torch.manual_seed(params.random_seed)\n",
    "\n",
    "dataset = make_dataset(params.p)\n",
    "train_data, test_data = train_test_split(dataset, params.train_frac, params.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints model parameter count\n",
    "model = Medium_Large(params).to(params.device)\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpointed_models, df = train(\n",
    "    train_dataset=train_data, test_dataset=test_data, params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "U4NIjv3oeP9s",
    "outputId": "957d56b3-e487-4665-f713-6e307e96830f"
   },
   "outputs": [],
   "source": [
    "plt.plot(df[\"val_acc\"], label=\"test\")\n",
    "plt.plot(df[\"train_acc\"], label=\"train\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Correct answer %\")\n",
    "plt.xlabel(\"Checkpoint\")\n",
    "plt.title(f\"Train & test correct answer % for modular addition with p={params.p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "LJ25wEvreP9w",
    "outputId": "06fcfff0-59f3-49b9-eec0-c129c7a6c0f4"
   },
   "outputs": [],
   "source": [
    "plt.plot(df[\"val_loss\"], label=\"test\")\n",
    "plt.plot(df[\"train_loss\"], label=\"train\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Checkpoint\")\n",
    "plt.title(f\"Train & test loss for modular addition with p={params.p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WvYf-UZeP92"
   },
   "source": [
    "## LLC estimation hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a sweep across several hyperparameters to try to estimate ones that will give us good LLC estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCGWHjNkeP96"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Type\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def estimate_llc_given_model(\n",
    "    model: torch.nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    evaluate: typing.Callable,\n",
    "    epsilon: float,\n",
    "    beta: float,\n",
    "    sampling_method: Type[torch.optim.Optimizer] = SGLD,\n",
    "    localization: float = 5.0,\n",
    "    num_chains: int = 2,\n",
    "    num_draws: int = 500,\n",
    "    num_burnin_steps: int = 0,\n",
    "    num_steps_bw_draws: int = 1,\n",
    "    device: torch.device = DEVICE,\n",
    "    online: bool = True,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    sweep_stats = estimate_learning_coeff_with_summary(\n",
    "        model,\n",
    "        loader=loader,\n",
    "        evaluate=evaluate,\n",
    "        sampling_method=sampling_method,\n",
    "        optimizer_kwargs=dict(lr=epsilon, localization=localization, nbeta=beta),\n",
    "        num_chains=num_chains,  # How many independent chains to run\n",
    "        num_draws=num_draws,  # How many samples to draw per chain\n",
    "        num_burnin_steps=num_burnin_steps,  # How many samples to discard at the beginning of each chain\n",
    "        num_steps_bw_draws=num_steps_bw_draws,  # How many steps to take between each sample\n",
    "        device=device,\n",
    "        online=online,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    sweep_stats[\"llc/trace\"] = np.array(sweep_stats[\"llc/trace\"])\n",
    "    return sweep_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NnCyNGueP99",
    "outputId": "388e75e3-dadb-4c9d-9fed-88bac739fa88"
   },
   "outputs": [],
   "source": [
    "from devinterp.vis_utils import EpsilonBetaAnalyzer\n",
    "\n",
    "loader = DataLoader(train_data, shuffle=True, batch_size=params.batch_size)\n",
    "analyzer = EpsilonBetaAnalyzer()\n",
    "analyzer.configure_sweep(\n",
    "    llc_estimator=estimate_llc_given_model,\n",
    "    llc_estimator_kwargs=dict(\n",
    "        model=all_checkpointed_models[-1],\n",
    "        evaluate=evaluate_ce,\n",
    "        device=DEVICE,\n",
    "        loader=loader,\n",
    "    ),\n",
    "    min_epsilon=3e-5,\n",
    "    max_epsilon=3e-1,\n",
    "    epsilon_samples=5,\n",
    "    min_beta=None,\n",
    "    max_beta=None,\n",
    "    beta_samples=5,\n",
    "    dataloader=loader,\n",
    ")\n",
    "analyzer.sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "TTP1vX3EeP9_",
    "outputId": "5a42fc43-901c-4001-9b49-6af99cd5eb2a"
   },
   "outputs": [],
   "source": [
    "analyzer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "qT3R92hweP-B",
    "outputId": "38e3e86e-c5c0-4fc6-a7ba-115f44a390c7"
   },
   "outputs": [],
   "source": [
    "analyzer.plot(div_out_beta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vVC0yMSeP-E"
   },
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "gamma = 5\n",
    "nbeta = 2.0\n",
    "num_draws = 75\n",
    "num_chains = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_bBq6sUXeP-H",
    "outputId": "e1dd4c1a-ce63-48e2-c7f8-2b0549c91c09"
   },
   "outputs": [],
   "source": [
    "learning_coeff_stats = estimate_learning_coeff_with_summary(\n",
    "    all_checkpointed_models[-1],\n",
    "    loader=DataLoader(train_data, batch_size=params.batch_size, shuffle=True),\n",
    "    evaluate=evaluate_ce,\n",
    "    sampling_method=SGLD,\n",
    "    optimizer_kwargs=dict(lr=0.03, nbeta=2.0, localization=5.0),\n",
    "    num_chains=3,\n",
    "    num_draws=1500,\n",
    "    device=DEVICE,\n",
    "    online=True,\n",
    ")\n",
    "trace = learning_coeff_stats[\"loss/trace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "U7A9q8PVeP-I",
    "outputId": "0e29f3c9-1b49-4a74-d4ee-93d187cda33d"
   },
   "outputs": [],
   "source": [
    "from devinterp.utils import plot_trace\n",
    "\n",
    "plot_trace(\n",
    "    trace,\n",
    "    \"Loss\",\n",
    "    x_axis=\"Step\",\n",
    "    title=f\"Loss Trace, avg LLC = {sum(learning_coeff_stats['llc/means']) / len(learning_coeff_stats['llc/means']):.2f}\",\n",
    "    plot_mean=False,\n",
    "    plot_std=False,\n",
    "    fig_size=(12, 9),\n",
    "    true_lc=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loss trace, it seems like we can get away with a very low draw count, around 75 should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llcs = [\n",
    "    estimate_learning_coeff_with_summary(\n",
    "        model_checkpoint,\n",
    "        loader=DataLoader(train_data, batch_size=params.batch_size, shuffle=True),\n",
    "        evaluate=evaluate_ce,\n",
    "        sampling_method=SGLD,\n",
    "        optimizer_kwargs=dict(lr=lr, nbeta=nbeta, localization=gamma),\n",
    "        num_chains=1,\n",
    "        num_draws=num_draws,\n",
    "        device=DEVICE,\n",
    "        online=False,\n",
    "    )\n",
    "    for model_checkpoint in all_checkpointed_models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_run_avg(arr, d):\n",
    "    run_avgs = [None for _ in range(d // 2)]\n",
    "\n",
    "    for i in range(len(arr) - d):\n",
    "        avg = sum(arr[i:i + d]) / d\n",
    "        run_avgs.append(avg)\n",
    "    \n",
    "    return run_avgs\n",
    "\n",
    "def calc_delta(arr, scale):\n",
    "    deltas = [None]\n",
    "\n",
    "    for i in range(1, len(arr)):\n",
    "        deltas.append((arr[i] - arr[i - 1]) * scale)\n",
    "    \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "r0pc1PKCeP-M",
    "outputId": "2a0c0a32-5edc-4e66-ac18-fc472fb92f6b"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "plt.title(\n",
    "    f\"Peak Loss 775934 params\"\n",
    ")\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(df[\"val_loss\"], label=\"test loss\")\n",
    "ax1.plot(df[\"train_loss\"], label=\"train loss\")\n",
    "# ax2.plot(calc_run_avg(delta_val_loss[1:], window_size), color=\"m\", label=\"Change in val acc\")\n",
    "ax2.plot([llc[\"llc/mean\"] for llc in llcs], color=\"g\", label=\"Lambdahat\")\n",
    "# ax2.plot(run_avg_llc, color=\"r\", label=\"Running avg llc\")\n",
    "ax1.set_xlabel(\"Checkpoint no.\")\n",
    "fig.legend(loc=\"center right\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
