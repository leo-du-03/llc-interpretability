{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wwbrhXCeP9Q"
   },
   "source": [
    "# Grokking\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timaeus-research/devinterp/blob/main/examples/grokking.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S3pemU_eP9W"
   },
   "source": [
    "This notebook aims to show how LLC estimation is calibrated in a simple modular addition grokking example, showing a moderately interesting result at the end.\n",
    "\n",
    "We'll starting off with some standard grokking code, adapted loosely from Nina Panickssery and Dmitry Vaintrob's [modular addition learning coefficient post](https://www.alignmentforum.org/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition) and [github code repo](https://github.com/nrimsky/devinterp). (Thank you for your help!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suJsw_j5eP9Y",
    "outputId": "17013fd2-6930-4305-af12-2a21e2feb98c"
   },
   "outputs": [],
   "source": [
    "%pip install devinterp nbformat\n",
    "%pip install devinterp[vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56I2aTSbeP9e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary\n",
    "from devinterp.utils import evaluate_mse\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvF8FyrLeP9g"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentParams:\n",
    "    vocab_size: int = 26 #size of vocabulary (a-z)\n",
    "    seq_length: int = 10 #length of input sequences\n",
    "    n_samples: int = 2000 #Total samples to generate (replaces p)\n",
    "    # p: int = 53 #prime modulus (learns addition mod 53) -> CHANGE to MY MODEL FRACTION X PREVIOUS TOKENS\n",
    "    n_batches: int = 50000 #Number of training steps was 25000\n",
    "    n_save_model_checkpoints: int = 100\n",
    "    print_times: int = 100\n",
    "    lr: float = 0.01 #learning rate #raise to help memorization for grokking\n",
    "    batch_size: int = 256 #batch size was 128\n",
    "    hidden_size: int = 48 #hidden layer size (was 48)\n",
    "    embed_dim: int = 64 #embedding dimension increased from 12\n",
    "    # train_frac: float = 0.4 #use 40% of data for training\n",
    "    train_frac: float = 0.3 #try 30% for grokking behavior more memorization\n",
    "    # the shown grokking / llc curve behavior is robust to change of seed from my experiments, but not all seeds show grokking withying the first 100 checkpoints, NB!\n",
    "    random_seed: int = 0\n",
    "    device: str = DEVICE\n",
    "    # weight_decay: float = 0.0002 #regularization\n",
    "    weight_decay: float = 0.1 # allow for more simpler solutions [generalization] to show grokking\n",
    "\n",
    "class MLP(nn.Module):  # Keep same name for compatibility\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(params.vocab_size, params.embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(params.seq_length, params.embed_dim)\n",
    "        \n",
    "        # Use TransformerEncoderLayer (like your teammate did)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=params.embed_dim,\n",
    "                nhead=4,  # Number of attention heads\n",
    "                dim_feedforward=params.hidden_size,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(2)  # 2 layers\n",
    "        ])\n",
    "        \n",
    "        # Output: single value per position (the fraction)\n",
    "        self.output = nn.Linear(params.embed_dim, 1)\n",
    "        #initialize larger weights\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal_(p, gain=2.0) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len) - tensor of token indices\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embed tokens\n",
    "        token_embeds = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_embeds = self.pos_encoding(positions)\n",
    "        x = token_embeds + pos_embeds\n",
    "        \n",
    "        # Create causal mask (position i only sees 0...i)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "        \n",
    "        # Apply transformer layers with causal mask\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x, src_mask=causal_mask)\n",
    "        \n",
    "        # Output fraction at each position\n",
    "        x = self.output(x).squeeze(-1)  # (batch, seq_len)\n",
    "        return torch.sigmoid(x)  # Constrain to [0, 1]\n",
    "\n",
    "def test(model, dataset, device):\n",
    "    total_loss = 0\n",
    "    total_mae = 0  # Mean Absolute Error\n",
    "    total_positions = 0\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataset:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # x is (seq_len,), y is (seq_len,)\n",
    "            # Need to add batch dimension\n",
    "            x = x.unsqueeze(0)  # (1, seq_len)\n",
    "            y = y.unsqueeze(0)  # (1, seq_len)\n",
    "            \n",
    "            out = model(x)  # (1, seq_len)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate MAE (Mean Absolute Error)\n",
    "            mae = torch.abs(out - y).sum()\n",
    "            total_mae += mae.item()\n",
    "            total_positions += y.shape[1]  # Count all positions\n",
    "    \n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    avg_mae = total_mae / total_positions\n",
    "    \n",
    "    return avg_mae, avg_loss  # Return MAE instead of accuracy\n",
    "\n",
    "\n",
    "def train(train_dataset, test_dataset, params, verbose=True):\n",
    "    all_models = []\n",
    "    model = MLP(params).to(params.device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=params.weight_decay, lr=params.lr\n",
    "    )\n",
    "    # loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "    print_every = params.n_batches // params.print_times\n",
    "    checkpoint_every = None\n",
    "    if params.n_save_model_checkpoints > 0:\n",
    "        checkpoint_every = params.n_batches // params.n_save_model_checkpoints\n",
    "\n",
    "    loss_data = []\n",
    "    if verbose:\n",
    "        pbar = tqdm(total=params.n_batches, desc=\"Training\")\n",
    "    for i in range(params.n_batches):\n",
    "        # Sample random batch of data\n",
    "        batch = next(iter(train_loader))\n",
    "        print(batch)\n",
    "        X, Y = batch\n",
    "        X, Y = X.to(params.device), Y.to(params.device)\n",
    "        # Gradient update\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = loss_fn(out, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if checkpoint_every and (i + 1) % checkpoint_every == 0:\n",
    "            all_models += [deepcopy(model)]\n",
    "\n",
    "        if (i + 1) % print_every == 0:\n",
    "            # val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "            val_mae, val_loss = test(model, test_dataset, params.device)\n",
    "            # train_acc, train_loss = test(model, train_dataset, params.device)\n",
    "            train_mae, train_loss = test(model, train_dataset, params.device)\n",
    "\n",
    "            loss_data.append(\n",
    "                {\n",
    "                    \"batch\": i + 1,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"train_mae\": train_mae,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_mae\": val_mae,\n",
    "                }\n",
    "            )\n",
    "            if verbose:\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": f\"{train_loss:.4f}\",\n",
    "                        \"train_acc\": f\"{train_mae:.4f}\", #changed to mae\n",
    "                        \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                        \"val_acc\": f\"{val_mae:.4f}\", #changed to mae\n",
    "                    }\n",
    "                )\n",
    "                pbar.update(print_every)\n",
    "    if verbose:\n",
    "        pbar.close()\n",
    "    df = pd.DataFrame(loss_data)\n",
    "    train_mae, train_loss = test(model, train_dataset, params.device) #changes to mae\n",
    "    val_mae, val_loss = test(model, test_dataset, params.device) #changed to mae\n",
    "    if verbose:\n",
    "        print(f\"Final Train Acc: {train_mae:.4f} | Final Train Loss: {train_loss:.4f}\") #changed to mae\n",
    "        print(f\"Final Val Acc: {val_mae:.4f} | Final Val Loss: {val_loss:.4f}\") #changed to mae\n",
    "    return all_models, df\n",
    "\n",
    "\n",
    "def deterministic_shuffle(lst, seed):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lst)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def get_all_pairs(p):\n",
    "    pairs = []\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            pairs.append((i, j))\n",
    "    return set(pairs)\n",
    "\n",
    "#CHANGE 1 - fraction of x's at each position\n",
    "def make_dataset(p):  # Keep p parameter name for compatibility\n",
    "    data = []\n",
    "    vocab = list(\"abcdefghijklmnopqrstuvwxyz\"[:26])\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    x_token = \"x\"\n",
    "    x_idx = char_to_idx[x_token]\n",
    "    seq_length = 10\n",
    "    \n",
    "    for _ in range(p):\n",
    "        # Generate random sequence - GUARANTEE at least 1 x\n",
    "        num_x = random.randint(0, seq_length // 2)  # 1 to 5 x's\n",
    "        \n",
    "        # Create sequence with x's and other letters\n",
    "        seq_chars = ['x'] * num_x\n",
    "        for _ in range(seq_length - num_x):\n",
    "            # Pick random letter that's not 'x'\n",
    "            other_letters = [c for c in vocab if c != 'x']\n",
    "            seq_chars.append(random.choice(other_letters))\n",
    "        \n",
    "        # Shuffle so x's are distributed randomly\n",
    "        random.shuffle(seq_chars)\n",
    "        \n",
    "        # Convert to indices\n",
    "        seq = [char_to_idx[c] for c in seq_chars]\n",
    "        \n",
    "        # Calculate fraction of x's at each position\n",
    "        targets = []\n",
    "        x_count = 0\n",
    "        for i, token_idx in enumerate(seq):\n",
    "            if token_idx == x_idx:\n",
    "                x_count += 1\n",
    "            fraction = x_count / (i + 1)\n",
    "            targets.append(fraction)\n",
    "        \n",
    "        data.append((torch.tensor(seq), torch.tensor(targets, dtype=torch.float32)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_test_split(dataset, train_split_proportion, seed):\n",
    "    l = len(dataset)\n",
    "    train_len = int(train_split_proportion * l)\n",
    "    idx = list(range(l))\n",
    "    idx = deterministic_shuffle(idx, seed)\n",
    "    train_idx = idx[:train_len]\n",
    "    test_idx = idx[train_len:]\n",
    "    return [dataset[i] for i in train_idx], [dataset[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xVu2IN7eP9o",
    "outputId": "da84ba29-4b2a-46c7-d4ea-9f5c4f338366"
   },
   "outputs": [],
   "source": [
    "params = ExperimentParams()\n",
    "torch.manual_seed(params.random_seed) #reproducibility\n",
    "\n",
    "# dataset = make_dataset(params.p) # All 53*53 = 2809 pairs\n",
    "dataset = make_dataset(params.n_samples) #changed to n_samples\n",
    "train_data, test_data = train_test_split(dataset, params.train_frac, params.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpointed_models, df = train(\n",
    "    train_dataset=train_data, test_dataset=test_data, params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "U4NIjv3oeP9s",
    "outputId": "957d56b3-e487-4665-f713-6e307e96830f"
   },
   "outputs": [],
   "source": [
    "plt.plot(df[\"val_mae\"], label=\"test\")  # Changed from val_acc\n",
    "plt.plot(df[\"train_mae\"], label=\"train\")  # Changed from train_acc\n",
    "plt.legend()\n",
    "plt.ylabel(\"Mean Absolute Error\")  # Changed from \"Correct answer %\"\n",
    "plt.xlabel(\"Checkpoint\")\n",
    "plt.title(f\"Train & test MAE for fraction-of-x with vocab_size={params.vocab_size}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dII4gLo6eP9u"
   },
   "source": [
    "From this plot, we see the classic grokking behavior: although the train accuracy is perfect after a few iterations, it takes many more examples for the test accuracy to meaningfully improve. (Note that this is not the same statement as train loss being perfect, see below plot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "LJ25wEvreP9w",
    "outputId": "06fcfff0-59f3-49b9-eec0-c129c7a6c0f4"
   },
   "outputs": [],
   "source": [
    "plt.plot(df[\"val_loss\"], label=\"test\")\n",
    "plt.plot(df[\"train_loss\"], label=\"train\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Checkpoint\")\n",
    "plt.title(f\"Train & test loss for modular addition with vocab={params.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataset\n",
    "test_sample = dataset[0]\n",
    "x, y = test_sample\n",
    "print(f\"Input indices: {x}\")\n",
    "print(f\"Target fractions: {y}\")\n",
    "print(f\"\\nConverting back to chars:\")\n",
    "vocab = list(\"abcdefghijklmnopqrstuvwxyz\"[:26])\n",
    "chars = [vocab[i] for i in x]\n",
    "print(f\"Sequence: {chars}\")\n",
    "print(f\"Targets:  {[f'{v:.3f}' for v in y.tolist()]}\")\n",
    "\n",
    "# Manual verification\n",
    "x_count = 0\n",
    "for i, char in enumerate(chars):\n",
    "    if char == 'x':\n",
    "        x_count += 1\n",
    "    expected = x_count / (i + 1)\n",
    "    print(f\"Position {i}: '{char}' -> x_count={x_count}, expected={expected:.3f}, got={y[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WvYf-UZeP92"
   },
   "source": [
    "## LLC estimation hyperparameter tuning (THIS HAS NOT BEEN IMPLEMENTED FOR FRACTION TOKEN YET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP4h1UJEeP94"
   },
   "source": [
    "In order to get LLC estimates for this simple grokking model over training, we first need to choose hyperparameters. The most important ones to calibrate are epsilon (the SGLD learning rate / step size) and n\\*beta (the effective inverse temperature). Let's run a quick sweep over a wide range of epsilon and n\\*beta, and look for a range of values within this which shows little change in LLC change in LLC values when we change epsilon and nbeta. We can use `devinterp.vis_utils.EpsilonBetaAnalyzer` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCGWHjNkeP96"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Type\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def estimate_llc_given_model(\n",
    "    model: torch.nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    evaluate: typing.Callable,\n",
    "    epsilon: float,\n",
    "    beta: float,\n",
    "    sampling_method: Type[torch.optim.Optimizer] = SGLD,\n",
    "    localization: float = 5.0,\n",
    "    num_chains: int = 2,\n",
    "    num_draws: int = 500,\n",
    "    num_burnin_steps: int = 0,\n",
    "    num_steps_bw_draws: int = 1,\n",
    "    device: torch.device = DEVICE,\n",
    "    online: bool = True,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    sweep_stats = estimate_learning_coeff_with_summary(\n",
    "        model,\n",
    "        loader=loader,\n",
    "        evaluate=evaluate,\n",
    "        sampling_method=sampling_method,\n",
    "        optimizer_kwargs=dict(lr=epsilon, localization=localization, nbeta=beta),\n",
    "        num_chains=num_chains,  # How many independent chains to run\n",
    "        num_draws=num_draws,  # How many samples to draw per chain\n",
    "        num_burnin_steps=num_burnin_steps,  # How many samples to discard at the beginning of each chain\n",
    "        num_steps_bw_draws=num_steps_bw_draws,  # How many steps to take between each sample\n",
    "        device=device,\n",
    "        online=online,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    sweep_stats[\"llc/trace\"] = np.array(sweep_stats[\"llc/trace\"])\n",
    "    return sweep_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NnCyNGueP99",
    "outputId": "388e75e3-dadb-4c9d-9fed-88bac739fa88"
   },
   "outputs": [],
   "source": [
    "from devinterp.vis_utils import EpsilonBetaAnalyzer\n",
    "\n",
    "loader = DataLoader(train_data, shuffle=True, batch_size=params.batch_size)\n",
    "analyzer = EpsilonBetaAnalyzer()\n",
    "analyzer.configure_sweep(\n",
    "    llc_estimator=estimate_llc_given_model,\n",
    "    llc_estimator_kwargs=dict(\n",
    "        model=all_checkpointed_models[-1],\n",
    "        evaluate=evaluate_mse,\n",
    "        device=DEVICE,\n",
    "        loader=loader,\n",
    "    ),\n",
    "    min_epsilon=3e-5,\n",
    "    max_epsilon=3e-1,\n",
    "    epsilon_samples=5,\n",
    "    min_beta=None,\n",
    "    max_beta=None,\n",
    "    beta_samples=5,\n",
    "    dataloader=loader,\n",
    ")\n",
    "analyzer.sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "TTP1vX3EeP9_",
    "outputId": "5a42fc43-901c-4001-9b49-6af99cd5eb2a"
   },
   "outputs": [],
   "source": [
    "analyzer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhWkZ18xeP-B"
   },
   "source": [
    "From this, we can see that the final LLC flattens out if epsilon > 0.001, so that's the epsilon parameter range we should go for. But we also have some dependence of the llc on beta, which is maybe linear from the looks of it? We get our LLC estimates by taking (sampled_loss - initial_loss) * nbeta, so maybe that final nbeta term is what we're seeing here. Let's divide it out to see this better.\n",
    "\n",
    "(Note that this does not quite mean the LLC curve should be fully linear in nbeta, as the choice of nbeta can and does influence the SGLD sampling process and so can change the sampled loss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "qT3R92hweP-B",
    "outputId": "38e3e86e-c5c0-4fc6-a7ba-115f44a390c7"
   },
   "outputs": [],
   "source": [
    "analyzer.plot(div_out_beta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlGfy4ZAeP-C"
   },
   "source": [
    "From this, we can see that the effective sampled loss for low-ish nbetas (<100) shows very little dependence on the exact choice of nbeta. So let's a point in this flat region (~1), and a high-but-still-in-the-flat-region epsilon (0.03), so we don't need to run many draws, but still have little dependence of our samples on epsilon.\n",
    "\n",
    "Let's check that the loss chain for these hyperparams looks decent, and then run LLC estimation on all trained checkpoints if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vVC0yMSeP-E"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "gamma = 5\n",
    "nbeta = 2.0\n",
    "num_draws = 75\n",
    "num_chains = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_bBq6sUXeP-H",
    "outputId": "e1dd4c1a-ce63-48e2-c7f8-2b0549c91c09"
   },
   "outputs": [],
   "source": [
    "learning_coeff_stats = estimate_learning_coeff_with_summary(\n",
    "    all_checkpointed_models[-1],\n",
    "    loader=DataLoader(train_data, batch_size=params.batch_size, shuffle=True),\n",
    "    evaluate=evaluate_mse,\n",
    "    sampling_method=SGLD,\n",
    "    optimizer_kwargs=dict(lr=0.03, nbeta=2.0, localization=5.0),\n",
    "    num_chains=3,\n",
    "    num_draws=1500,\n",
    "    device=DEVICE,\n",
    "    online=True,\n",
    ")\n",
    "trace = learning_coeff_stats[\"loss/trace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "U7A9q8PVeP-I",
    "outputId": "0e29f3c9-1b49-4a74-d4ee-93d187cda33d"
   },
   "outputs": [],
   "source": [
    "from devinterp.utils import plot_trace\n",
    "\n",
    "plot_trace(\n",
    "    trace,\n",
    "    \"Loss\",\n",
    "    x_axis=\"Step\",\n",
    "    title=f\"Loss Trace, avg LLC = {sum(learning_coeff_stats['llc/means']) / len(learning_coeff_stats['llc/means']):.2f}\",\n",
    "    plot_mean=False,\n",
    "    plot_std=False,\n",
    "    fig_size=(12, 9),\n",
    "    true_lc=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj1Jr9kIeP-K"
   },
   "source": [
    "This looks good! The loss flattens out nicely, and well within the num_draws we chose. Looks like we can get away with using 500 draws, as the loss trace has well flattened out by then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJfjqhXdeP-L",
    "outputId": "017daa08-db49-4f39-ea1f-2da1e9a4aa86"
   },
   "outputs": [],
   "source": [
    "llcs = [\n",
    "    estimate_learning_coeff_with_summary(\n",
    "        model_checkpoint,\n",
    "        loader=DataLoader(train_data, batch_size=params.batch_size, shuffle=True),\n",
    "        evaluate=evaluate_mse,\n",
    "        sampling_method=SGLD,\n",
    "        optimizer_kwargs=dict(lr=lr, nbeta=nbeta, localization=gamma),\n",
    "        num_chains=1,\n",
    "        num_draws=num_draws,\n",
    "        device=DEVICE,\n",
    "        online=False,\n",
    "    )\n",
    "    for model_checkpoint in all_checkpointed_models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "r0pc1PKCeP-M",
    "outputId": "2a0c0a32-5edc-4e66-ac18-fc472fb92f6b"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "plt.title(\n",
    "    f\"Lambdahat vs acc for fraction token vocab={params.vocab_size}, train_frac={params.train_frac}, nβ={nbeta:.1f}, ε={lr}, γ={gamma}, num_draws={num_draws}, num_chains={num_chains}\"\n",
    ")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(df[\"val_mae\"], label=\"test mae\")\n",
    "ax1.plot(df[\"train_mae\"], label=\"train mae\")\n",
    "ax2.plot([llc[\"llc/mean\"] for llc in llcs], color=\"g\", label=\"Lambdahat\")\n",
    "ax1.set_xlabel(\"Checkpoint no.\")\n",
    "fig.legend(loc=\"center right\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.title(\n",
    "    f\"Lambdahat vs loss for fraction token, vocab={params.vocab_size}, train_frac={params.train_frac}, nβ={nbeta:.1f}, ε={lr}, γ={gamma}, num_draws={num_draws}, num_chains={num_chains}\"\n",
    ")\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(df[\"val_mae\"], label=\"test mae\")\n",
    "ax1.plot(df[\"train_mae\"], label=\"train mae\")\n",
    "ax2.plot([llc[\"llc/mean\"] for llc in llcs], color=\"g\", label=\"Lambdahat\")\n",
    "ax1.set_xlabel(\"Checkpoint no.\")\n",
    "fig.legend(loc=\"center right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhnimJrWeP-Q"
   },
   "source": [
    "That's interesting!\n",
    "\n",
    "In the first plot, we see that the LLC first increases during memorization and then decreases ~smoothly afterward, flattening out after the model is done grokking. This is basically what we would expect from a simple reading of phase transitions in the free energy formula.\n",
    "\n",
    "From the second plot, we see that the LLC, which was measured only on the train set, tracks the test loss pretty well. That was a big surprise for me when I made this notebook, and I don't know what it means.\n",
    "\n",
    "Anyway, I hope this notebook clarifies how one can use the devinterp library and LLC estimation more generally to gain insight in the development of structure in neural networks. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
